#!/usr/bin/env python3
"""
YouTube to Clean Audio-Based Script Generator
============================================

Generates perfectly clean two-speaker conversational scripts from YouTube videos
using direct audio processing with Whisper + speaker diarization.

This approach provides:
- 100% accurate transcription (Whisper)
- True speaker identification (not pattern-based guessing)
- Perfect word-level timestamps
- No caption artifacts
- Complete control over the pipeline

Usage:
    python youtube-to-audio-script.py "https://www.youtube.com/watch?v=abc123"
    python youtube-to-audio-script.py "https://www.youtube.com/watch?v=abc123" --speakers "Dr. James,Sarah"

Dependencies:
    pip install whisper-timestamped yt-dlp pyannote.audio torch
"""

import os
import sys
import argparse
import re
import subprocess
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
import tempfile
import json
from datetime import datetime

try:
    import whisper_timestamped as whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False
    print("‚ùå whisper-timestamped required: pip install whisper-timestamped")

try:
    import yt_dlp
    YTDLP_AVAILABLE = True
except ImportError:
    YTDLP_AVAILABLE = False
    print("‚ùå yt-dlp required: pip install yt-dlp")

try:
    from pyannote.audio import Pipeline
    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
    print("‚ö†Ô∏è  pyannote.audio recommended for speaker diarization: pip install pyannote.audio")

class YouTubeAudioScriptGenerator:
    """Generates clean two-speaker scripts using direct audio processing"""
    
    def __init__(self, output_dir: str = "audio_scripts"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        self.temp_dir = Path(tempfile.mkdtemp(prefix="audio_script_"))
        print(f"üîß Working in: {self.temp_dir}")
        
        # Load Whisper model (large-v2 for best quality)
        print("ü§ñ Loading Whisper model (this may take a moment)...")
        self.whisper_model = whisper.load_model("large-v2")
        print("‚úÖ Whisper model loaded")
    
    def extract_video_info(self, url: str) -> Optional[Dict]:
        """Extract video metadata"""
        print(f"üì∫ Extracting video info...")
        
        ydl_opts = {'quiet': True, 'no_warnings': True}
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            try:
                info = ydl.extract_info(url, download=False)
                video_info = {
                    'title': info.get('title', 'Unknown Title'),
                    'uploader': info.get('uploader', 'Unknown Channel'),
                    'duration': info.get('duration', 0),
                    'video_id': info.get('id', '')
                }
                
                print(f"‚úÖ {video_info['title']} ({video_info['duration']//60}:{video_info['duration']%60:02d})")
                return video_info
                
            except Exception as e:
                print(f"‚ùå Failed: {e}")
                return None
    
    def download_audio(self, url: str, video_info: Dict) -> Optional[Path]:
        """Download high-quality audio from YouTube"""
        print(f"üéµ Downloading audio...")
        
        audio_file = self.temp_dir / f"{video_info['video_id']}.wav"
        
        # Download best audio quality
        ydl_opts = {
            'format': 'bestaudio/best',
            'outtmpl': str(self.temp_dir / f"{video_info['video_id']}.%(ext)s"),
            'postprocessors': [{
                'key': 'FFmpegExtractAudio',
                'preferredcodec': 'wav',
                'preferredquality': '192',
            }],
            'quiet': True,
        }
        
        try:
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download([url])
            
            # Find the downloaded wav file
            wav_files = list(self.temp_dir.glob("*.wav"))
            if wav_files:
                # Rename to standard name if needed
                if wav_files[0] != audio_file:
                    wav_files[0].rename(audio_file)
                print(f"‚úÖ Audio downloaded: {audio_file.name}")
                return audio_file
            else:
                print("‚ùå No audio file found after download")
                return None
                
        except Exception as e:
            print(f"‚ùå Audio download failed: {e}")
            return None
    
    def transcribe_with_whisper(self, audio_file: Path) -> Optional[Dict]:
        """Transcribe audio using Whisper with word-level timestamps"""
        print(f"üéôÔ∏è Transcribing with Whisper (this may take several minutes)...")
        
        try:
            # Use whisper-timestamped for word-level timestamps
            result = whisper.transcribe(
                self.whisper_model, 
                str(audio_file),
                language="en",
                verbose=False
            )
            
            print(f"‚úÖ Transcription complete ({len(result.get('segments', []))} segments)")
            return result
            
        except Exception as e:
            print(f"‚ùå Transcription failed: {e}")
            return None
    
    def perform_speaker_diarization(self, audio_file: Path) -> Optional[Dict]:
        """Perform professional speaker diarization using pyannote.audio"""
        print(f"üë• Performing professional speaker diarization...")
        
        if not PYANNOTE_AVAILABLE:
            print("‚ö†Ô∏è  Using fallback speaker detection (install pyannote.audio for better results)")
            return None
        
        # Try to use proper pyannote models with authentication
        return self._pyannote_speaker_detection(audio_file) or self._simple_audio_speaker_detection(audio_file)
    
    def _pyannote_speaker_detection(self, audio_file: Path) -> Optional[Dict]:
        """Professional speaker diarization using pyannote.audio models"""
        try:
            import os
            from pyannote.audio import Pipeline
            
            # Check for HuggingFace token
            token = os.environ.get('HUGGINGFACE_HUB_TOKEN') or os.environ.get('HF_TOKEN')
            
            if not token:
                print("üí° No HuggingFace token found. Set HUGGINGFACE_HUB_TOKEN environment variable")
                print("   Visit: https://huggingface.co/settings/tokens")
                return None
            
            # Try the best available models
            model_names = [
                "pyannote/speaker-diarization-3.1",
                "pyannote/speaker-diarization-3.0", 
                "pyannote/speaker-diarization"
            ]
            
            pipeline = None
            for model_name in model_names:
                try:
                    print(f"üîê Loading {model_name} with authentication...")
                    pipeline = Pipeline.from_pretrained(model_name, use_auth_token=token)
                    print(f"‚úÖ Successfully loaded {model_name}")
                    break
                except Exception as e:
                    print(f"‚ö†Ô∏è  Failed to load {model_name}: {str(e)[:100]}...")
                    continue
            
            if pipeline is None:
                print("‚ùå Could not load any pyannote models")
                return None
            
            # Apply professional speaker diarization with 2-speaker constraint
            print("üéØ Analyzing audio for speaker changes...")
            diarization = pipeline(str(audio_file), num_speakers=2)
            
            # Convert to our format
            speaker_segments = []
            for turn, _, speaker in diarization.itertracks(yield_label=True):
                speaker_segments.append({
                    'start': turn.start,
                    'end': turn.end,
                    'speaker': speaker
                })
            
            print(f"‚úÖ Professional speaker diarization complete ({len(speaker_segments)} segments)")
            
            # Show speaker statistics
            speakers = set(seg['speaker'] for seg in speaker_segments)
            print(f"üé≠ Detected {len(speakers)} speakers: {', '.join(speakers)}")
            
            # If still more than 2 speakers, merge similar ones
            if len(speakers) > 2:
                print("üîß Consolidating to exactly 2 speakers...")
                speaker_segments = self._consolidate_to_two_speakers(speaker_segments)
                speakers = set(seg['speaker'] for seg in speaker_segments)
                print(f"‚úÖ Consolidated to {len(speakers)} speakers: {', '.join(speakers)}")
            
            return {'segments': speaker_segments}
            
        except ImportError:
            print("‚ùå pyannote.audio not available")
            return None
        except Exception as e:
            print(f"‚ùå Professional speaker diarization failed: {e}")
            return None
    
    def _consolidate_to_two_speakers(self, speaker_segments: List[Dict]) -> List[Dict]:
        """Consolidate multiple detected speakers into exactly 2 speakers"""
        from collections import defaultdict
        
        # Group segments by speaker
        speaker_durations = defaultdict(float)
        for seg in speaker_segments:
            speaker_durations[seg['speaker']] += seg['end'] - seg['start']
        
        # Find the two speakers with most speaking time
        top_speakers = sorted(speaker_durations.items(), key=lambda x: x[1], reverse=True)[:2]
        main_speaker, secondary_speaker = top_speakers[0][0], top_speakers[1][0]
        
        # Map all speakers to one of the two main speakers
        speaker_mapping = {}
        for speaker, duration in speaker_durations.items():
            if speaker == main_speaker:
                speaker_mapping[speaker] = main_speaker
            elif speaker == secondary_speaker:
                speaker_mapping[speaker] = secondary_speaker
            else:
                # Assign minority speakers to the closest main speaker
                # For simplicity, assign to secondary speaker
                speaker_mapping[speaker] = secondary_speaker
        
        # Apply mapping
        consolidated_segments = []
        for seg in speaker_segments:
            new_seg = seg.copy()
            new_seg['speaker'] = speaker_mapping[seg['speaker']]
            consolidated_segments.append(new_seg)
        
        return consolidated_segments
    
    def _simple_audio_speaker_detection(self, audio_file: Path) -> Optional[Dict]:
        """Simple speaker detection based on audio energy and pause patterns"""
        try:
            import librosa
            
            # Load audio file
            y, sr = librosa.load(str(audio_file), sr=16000)
            
            # Calculate audio features for speaker detection
            # 1. RMS Energy (volume changes often indicate speaker changes)
            hop_length = 512
            frame_length = 2048
            rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # 2. Zero crossing rate (voice characteristics)
            zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]
            
            # 3. Spectral centroid (voice pitch characteristics)
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)[0]
            
            # Convert frame indices to time
            times = librosa.frames_to_time(range(len(rms)), sr=sr, hop_length=hop_length)
            
            # Simple speaker change detection based on significant changes in features
            speaker_segments = []
            current_speaker = "SPEAKER_00"
            segment_start = 0
            
            # Detect speaker changes based on energy and feature changes
            for i in range(1, len(rms)):
                # Calculate feature differences
                rms_diff = abs(rms[i] - rms[i-1])
                zcr_diff = abs(zcr[i] - zcr[i-1])
                spectral_diff = abs(spectral_centroid[i] - spectral_centroid[i-1])
                
                # Normalize differences
                rms_threshold = 0.02  # Adjust based on testing
                zcr_threshold = 0.05
                spectral_threshold = 200
                
                # Detect potential speaker change
                if (rms_diff > rms_threshold or zcr_diff > zcr_threshold or spectral_diff > spectral_threshold):
                    # Check if enough time has passed (avoid too frequent changes)
                    if times[i] - segment_start > 2.0:  # At least 2 seconds
                        # End current segment
                        speaker_segments.append({
                            'start': segment_start,
                            'end': times[i],
                            'speaker': current_speaker
                        })
                        
                        # Start new segment with different speaker
                        current_speaker = "SPEAKER_01" if current_speaker == "SPEAKER_00" else "SPEAKER_00"
                        segment_start = times[i]
            
            # Add final segment
            if segment_start < times[-1]:
                speaker_segments.append({
                    'start': segment_start,
                    'end': times[-1],
                    'speaker': current_speaker
                })
            
            print(f"‚úÖ Simple speaker detection complete ({len(speaker_segments)} segments)")
            return {'segments': speaker_segments}
            
        except ImportError:
            print("‚ö†Ô∏è  librosa not available for audio analysis")
            return None
        except Exception as e:
            print(f"‚ùå Simple speaker detection failed: {e}")
            return None
    
    def align_speakers_with_transcription(self, transcription: Dict, diarization: Optional[Dict], 
                                        speaker_names: Tuple[str, str]) -> List[Dict]:
        """Align speaker diarization with word-level transcription"""
        print(f"üîó Aligning speakers with transcription...")
        
        aligned_segments = []
        
        if diarization is None:
            # Fallback: Use our existing pattern-based detection
            return self._fallback_speaker_detection(transcription, speaker_names)
        
        # FIRST: Determine correct gender mapping based on content analysis
        speaker_segments = diarization['segments']
        self._speaker_gender_mapping = self._determine_gender_mapping(speaker_segments, transcription, speaker_names)
        
        # Get all words with timestamps from Whisper
        all_words = []
        for segment in transcription.get('segments', []):
            for word_info in segment.get('words', []):
                all_words.append({
                    'word': word_info['text'],
                    'start': word_info['start'],
                    'end': word_info['end']
                })
        
        # Align diarization with words (now using correct gender mapping)
        current_segment = {'speaker': None, 'words': [], 'start': None, 'end': None}
        
        for word_info in all_words:
            word_start = word_info['start']
            word_end = word_info['end']
            
            # Find which speaker segment this word belongs to with improved timing
            assigned_speaker = None
            min_distance = float('inf')
            best_match = None
            
            for seg in speaker_segments:
                # Check if word is within segment boundaries (with generous tolerance for transitions)
                tolerance = 0.3  # 300ms tolerance for better boundary detection
                
                # For words at segment boundaries, prioritize the segment that STARTS closer to the word
                if word_start >= seg['start'] - tolerance and word_start <= seg['end'] + tolerance:
                    # Calculate distances to both start and end of segment
                    start_distance = abs(word_start - seg['start'])
                    end_distance = abs(word_start - seg['end'])
                    
                    # If word is very close to the START of a segment, prefer this segment
                    if start_distance <= 0.5:  # Within 500ms of segment start (more aggressive)
                        if start_distance < min_distance:
                            min_distance = start_distance
                            best_match = seg
                            assigned_speaker = seg['speaker']
                    # If word is clearly within the segment (not near boundaries)
                    elif word_start > seg['start'] + 0.2 and word_start < seg['end'] - 0.2:
                        assigned_speaker = seg['speaker']
                        break
                    # Otherwise consider it for best match
                    else:
                        combined_distance = min(start_distance, end_distance)
                        if combined_distance < min_distance:
                            min_distance = combined_distance
                            best_match = seg
            
            if assigned_speaker is None and best_match:
                assigned_speaker = best_match['speaker']
            
            if assigned_speaker is None:
                # Fallback: use nearest speaker segment
                min_distance = float('inf')
                for seg in speaker_segments:
                    distance = min(abs(word_start - seg['start']), abs(word_start - seg['end']))
                    if distance < min_distance:
                        min_distance = distance
                        assigned_speaker = seg['speaker']
            
            # Map speaker ID to name
            speaker_name = self._map_speaker_id_to_name(assigned_speaker, speaker_names)
            
            # Special handling for common interjections that might indicate speaker change
            word_text = word_info['word'].lower().strip('.,!?')
            is_interjection = word_text in ['that\'s', 'right', 'yes', 'yep', 'yeah', 'ok', 'okay', 'mm-hmm', 'uh-huh', 'exactly', 'sure']
            
            # If this is an interjection and we detect a different speaker, be more confident
            if is_interjection and current_segment['speaker'] and current_segment['speaker'] != speaker_name:
                # Check if this interjection is close to a segment boundary
                for seg in speaker_segments:
                    if abs(word_start - seg['start']) <= 0.3 and seg['speaker'] != current_segment['speaker']:
                        speaker_name = self._map_speaker_id_to_name(seg['speaker'], speaker_names)
                        break
            
            # Add word to running collection (we'll group by sentences, not individual words)
            # Store word with its assigned speaker for sentence-level processing
            word_info['assigned_speaker'] = speaker_name
        
        # Now process words into sentence-based segments
        aligned_segments = self._group_words_into_sentences(all_words, speaker_names)
        
        print(f"‚úÖ Alignment complete ({len(aligned_segments)} aligned segments)")
        return aligned_segments
    
    def _group_words_into_sentences(self, all_words: List[Dict], speaker_names: Tuple[str, str]) -> List[Dict]:
        """Group words into sentence-based segments, only allowing speaker changes at sentence boundaries"""
        sentence_segments = []
        current_sentence_words = []
        
        for i, word_info in enumerate(all_words):
            current_sentence_words.append(word_info)
            
            # Check if this word ends a sentence
            word_text = word_info['word'].strip()
            is_sentence_end = (
                word_text.endswith(('.', '!', '?')) or 
                i == len(all_words) - 1  # Last word
            )
            
            # Special handling for common short interjections that should be separate sentences
            if len(current_sentence_words) >= 2:  # At least 2 words accumulated
                sentence_so_far = ' '.join([w['word'] for w in current_sentence_words]).strip()
                
                # Check for complete interjection phrases
                interjection_phrases = ['that\'s right.', 'that\'s right,', 'exactly right.', 'yep,', 'yes,', 'absolutely.']
                sentence_lower = sentence_so_far.lower()
                
                # If we find a complete interjection phrase, end the sentence here
                for phrase in interjection_phrases:
                    if sentence_lower.endswith(phrase):
                        is_sentence_end = True
                        break
                
                # Also check for single-word interjections at end
                if (word_text.lower().strip('.,!?') in ['right', 'yep', 'yes', 'exactly', 'okay'] and
                    word_text.endswith(('.', ',', '!', '?')) and
                    len(current_sentence_words) <= 4):  # Short phrase
                    is_sentence_end = True
            
            if is_sentence_end and current_sentence_words:
                # Determine the primary speaker for this sentence
                sentence_speaker = self._determine_sentence_speaker(current_sentence_words, speaker_names)
                
                # Create sentence segment
                sentence_text = ' '.join([w['word'] for w in current_sentence_words])
                sentence_start = current_sentence_words[0]['start']
                sentence_end = current_sentence_words[-1]['end']
                
                sentence_segments.append({
                    'speaker': sentence_speaker,
                    'text': sentence_text.strip(),
                    'start': sentence_start,
                    'end': sentence_end
                })
                
                current_sentence_words = []
        
        # Handle any remaining incomplete sentence
        if current_sentence_words:
            sentence_speaker = self._determine_sentence_speaker(current_sentence_words, speaker_names)
            sentence_text = ' '.join([w['word'] for w in current_sentence_words])
            sentence_start = current_sentence_words[0]['start']
            sentence_end = current_sentence_words[-1]['end']
            
            sentence_segments.append({
                'speaker': sentence_speaker,
                'text': sentence_text.strip(),
                'start': sentence_start,
                'end': sentence_end
            })
        
        return sentence_segments
    
    def _determine_sentence_speaker(self, sentence_words: List[Dict], speaker_names: Tuple[str, str]) -> str:
        """Determine the primary speaker for a complete sentence"""
        if not sentence_words:
            return speaker_names[0]
        
        # Count speaker assignments for words in this sentence
        speaker_votes = {}
        word_count_by_speaker = {}
        
        for word_info in sentence_words:
            speaker = word_info.get('assigned_speaker', speaker_names[0])
            if speaker not in speaker_votes:
                speaker_votes[speaker] = 0
                word_count_by_speaker[speaker] = 0
            speaker_votes[speaker] += 1
            word_count_by_speaker[speaker] += 1
        
        # Special handling for short interjections at sentence start
        sentence_text = ' '.join([w['word'] for w in sentence_words]).strip()
        sentence_lower = sentence_text.lower()
        
        # Check if sentence starts with clear interjections that might indicate speaker change
        interjection_starters = ['that\'s right', 'that\'s', 'exactly', 'yes', 'yep', 'no', 'absolutely', 'right', 'okay']
        for interjection in interjection_starters:
            if sentence_lower.startswith(interjection):
                # For interjections, prefer the speaker assigned to the first few words
                first_few_words = sentence_words[:min(3, len(sentence_words))]
                first_speakers = [w.get('assigned_speaker', speaker_names[0]) for w in first_few_words]
                if first_speakers:
                    most_common_first = max(set(first_speakers), key=first_speakers.count)
                    return most_common_first
        
        # Otherwise, use majority vote for the sentence
        if not speaker_votes:
            return speaker_names[0]
        
        # Return speaker with most words in this sentence
        primary_speaker = max(speaker_votes.items(), key=lambda x: x[1])[0]
        return primary_speaker
    
    def _determine_gender_mapping(self, speaker_segments: List[Dict], transcription: Dict, 
                                 speaker_names: Tuple[str, str]) -> Dict[str, str]:
        """Determine which speaker ID corresponds to which gender by analyzing conversation content"""
        print("üîç Analyzing conversation content to determine speaker genders...")
        
        # Get all speaker IDs from diarization
        speaker_ids = list(set(seg['speaker'] for seg in speaker_segments))
        if len(speaker_ids) != 2:
            print(f"‚ö†Ô∏è  Expected 2 speakers, found {len(speaker_ids)}. Using default mapping.")
            return {speaker_ids[0]: speaker_names[0], speaker_ids[1] if len(speaker_ids) > 1 else speaker_ids[0]: speaker_names[1]}
        
        # Get text samples for each speaker ID from transcription
        speaker_texts = {speaker_id: [] for speaker_id in speaker_ids}
        
        # Collect text samples by analyzing which segments belong to which speaker
        for segment in transcription.get('segments', []):
            for word_info in segment.get('words', []):
                word_start = word_info['start']
                
                # Find which speaker segment this word belongs to
                for speaker_seg in speaker_segments:
                    if (speaker_seg['start'] <= word_start <= speaker_seg['end']):
                        speaker_id = speaker_seg['speaker']
                        speaker_texts[speaker_id].append(word_info['text'])
                        break
        
        # Analyze content patterns to determine gender roles
        speaker_analysis = {}
        for speaker_id, words in speaker_texts.items():
            text_sample = ' '.join(words[:100])  # First 100 words for analysis
            analysis = self._analyze_speaker_patterns(text_sample)
            speaker_analysis[speaker_id] = analysis
            print(f"   {speaker_id}: {analysis['role']} (confidence: {analysis['confidence']:.2f})")
        
        # Determine mapping based on analysis
        gender_mapping = {}
        
        # Sort speakers by host-like confidence (higher = more host-like)
        sorted_speakers = sorted(speaker_analysis.items(), 
                               key=lambda x: x[1]['host_score'], reverse=True)
        
        # Map based on typical podcast/interview structure
        # Typically: Male is often the host/interviewer, Female is often the expert/guest
        # But we analyze content to be sure
        host_speaker_id = sorted_speakers[0][0]  # Most host-like
        guest_speaker_id = sorted_speakers[1][0]  # Most guest-like
        
        # Analyze first few segments to see who starts the conversation
        first_speaker = None
        if speaker_segments:
            first_speaker = speaker_segments[0]['speaker']
        
        # Decision logic based on content analysis
        host_analysis = speaker_analysis[host_speaker_id]
        guest_analysis = speaker_analysis[guest_speaker_id]
        
        # If one speaker is clearly more host-like (welcomes, asks questions, manages conversation)
        if host_analysis['host_score'] > guest_analysis['host_score'] + 0.3:  # Clear host pattern
            # Host is often male in typical podcast format
            if "Male" in speaker_names[0] or "Dr." in speaker_names[0]:
                gender_mapping[host_speaker_id] = speaker_names[0]  # Male host
                gender_mapping[guest_speaker_id] = speaker_names[1]  # Female guest
            else:
                # Analyze actual content for gender cues
                gender_mapping = self._analyze_gender_from_content(speaker_analysis, speaker_names)
        else:
            # Similar host scores, use content-based gender analysis
            gender_mapping = self._analyze_gender_from_content(speaker_analysis, speaker_names)
        
        print(f"‚úÖ Gender mapping determined: {gender_mapping}")
        return gender_mapping
    
    def _analyze_speaker_patterns(self, text_sample: str) -> Dict:
        """Analyze text patterns to determine speaker role and characteristics"""
        text_lower = text_sample.lower()
        
        # Host/interviewer patterns
        host_patterns = [
            r'\b(welcome|today|let\'s|tell us|explain|help us understand)\b',
            r'\b(what|how|why|when|where)\b.*\?',  # Questions
            r'\b(that\'s interesting|sounds like|fascinating|amazing)\b',  # Reactions
            r'\b(so|now|next|moving on|let\'s talk about)\b',  # Transitions
        ]
        
        # Expert/guest patterns  
        expert_patterns = [
            r'\b(actually|basically|essentially|fundamentally|technically)\b',
            r'\b(the key|the important thing|what happens|you see|think of it)\b',
            r'\b(for example|specifically|in practice|the data shows)\b',
            r'\b(algorithm|function|variable|data|code|implementation)\b',  # Technical terms
        ]
        
        # Gender-neutral patterns (confidence/authority indicators)
        authority_patterns = [
            r'\b(obviously|clearly|definitely|absolutely|precisely)\b',
            r'\b(research shows|studies indicate|we found that)\b',
        ]
        
        # Count pattern matches
        host_score = sum(len(re.findall(pattern, text_lower)) for pattern in host_patterns)
        expert_score = sum(len(re.findall(pattern, text_lower)) for pattern in expert_patterns)
        authority_score = sum(len(re.findall(pattern, text_lower)) for pattern in authority_patterns)
        
        # Normalize by text length
        text_length = len(text_sample.split())
        if text_length > 0:
            host_score = host_score / text_length * 100
            expert_score = expert_score / text_length * 100
            authority_score = authority_score / text_length * 100
        
        # Determine role
        if host_score > expert_score:
            role = "host/interviewer"
            confidence = host_score / (host_score + expert_score + 0.01)
        else:
            role = "expert/guest" 
            confidence = expert_score / (host_score + expert_score + 0.01)
        
        return {
            'role': role,
            'host_score': host_score,
            'expert_score': expert_score,
            'authority_score': authority_score,
            'confidence': confidence
        }
    
    def _analyze_gender_from_content(self, speaker_analysis: Dict, speaker_names: Tuple[str, str]) -> Dict[str, str]:
        """Analyze gender based on content patterns and speaker names"""
        # This is a fallback - in a real implementation, you might use voice analysis
        # For now, we'll use the speaker who starts the conversation and content patterns
        
        speaker_ids = list(speaker_analysis.keys())
        
        # Simple heuristic: if one speaker is much more host-like, assign based on typical roles
        scores = [(sid, analysis['host_score']) for sid, analysis in speaker_analysis.items()]
        scores.sort(key=lambda x: x[1], reverse=True)
        
        host_speaker = scores[0][0]
        guest_speaker = scores[1][0]
        
        # Default assignment based on common podcast patterns
        # But this should be improved with actual voice analysis in the future
        gender_mapping = {}
        
        # Check speaker names to understand expected genders
        if "Male" in speaker_names and "Female" in speaker_names:
            male_name = next(name for name in speaker_names if "Male" in name)
            female_name = next(name for name in speaker_names if "Female" in name)
            
            # Assign based on first speaker (who usually welcomes/introduces)
            # This is a heuristic that may need adjustment based on your specific content
            gender_mapping[host_speaker] = male_name
            gender_mapping[guest_speaker] = female_name
        else:
            # Custom names - assign based on order
            gender_mapping[host_speaker] = speaker_names[0]
            gender_mapping[guest_speaker] = speaker_names[1]
        
        return gender_mapping
    
    def _map_speaker_id_to_name(self, speaker_id: str, speaker_names: Tuple[str, str]) -> str:
        """Map pyannote speaker ID to human-readable name - uses content-based mapping set by _determine_gender_mapping()"""
        # Use the gender mapping determined by content analysis
        if hasattr(self, '_speaker_gender_mapping') and self._speaker_gender_mapping:
            return self._speaker_gender_mapping.get(speaker_id, speaker_names[0])
        else:
            # Fallback to default mapping if content analysis hasn't run yet
            if speaker_id.endswith('0') or speaker_id == 'SPEAKER_00':
                return speaker_names[0]  # First speaker (default)
            else:
                return speaker_names[1]  # Second speaker (default)
    
    def _fallback_speaker_detection(self, transcription: Dict, speaker_names: Tuple[str, str]) -> List[Dict]:
        """Enhanced fallback speaker detection with conversation flow analysis"""
        print("üí° Using enhanced pattern-based speaker detection...")
        
        segments = []
        current_speaker = speaker_names[0]  # Start with first speaker (host)
        
        for i, segment in enumerate(transcription.get('segments', [])):
            text = segment['text'].strip()
            if not text:
                continue
            
            # Get context from previous segments
            prev_context = segments[-3:] if len(segments) >= 3 else segments
            
            # Enhanced speaker detection with context
            detected_speaker = self._detect_speaker_pattern_enhanced(
                text, current_speaker, speaker_names, prev_context
            )
            
            if detected_speaker:
                current_speaker = detected_speaker
            
            segments.append({
                'speaker': current_speaker,
                'text': text,
                'start': segment['start'],
                'end': segment['end']
            })
        
        return segments
    
    def _detect_speaker_pattern_enhanced(self, text: str, current_speaker: str, 
                                        speaker_names: Tuple[str, str], prev_context: List[Dict]) -> Optional[str]:
        """Enhanced pattern-based speaker detection with conversation flow analysis"""
        text_lower = text.lower().strip()
        
        # Short confirmatory responses usually indicate speaker change
        if len(text.split()) <= 3:
            confirmatory_responses = [
                r'^(that\'s right|that\'s correct|exactly|right|yes|absolutely|precisely|correct)$',
                r'^(true|indeed|definitely|sure|of course|yep|yeah)$',
                r'^(makes sense|good point|i see|got it|okay)$'
            ]
            if any(re.match(pattern, text_lower) for pattern in confirmatory_responses):
                # Switch to the other speaker
                return speaker_names[1] if current_speaker == speaker_names[0] else speaker_names[0]
        
        # Host patterns (questions, transitions, introductions)
        host_patterns = [
            r'^(what|how|why|when|where|who|can|could|would|should|do|does|did|is|are|was|were)\b',
            r'^(welcome|today|let\'s|okay|so|now|well|tell us|explain|help us understand)\b',
            r'^(that\'s interesting|sounds like|and that|moving on|next|finally)\b',
            r'^(wow|whoa|amazing|fascinating|interesting|great|perfect)\b',
            r'\?$',  # Questions
        ]
        
        # Expert patterns (explanations, technical content, detailed responses)
        expert_patterns = [
            r'^(well|actually|basically|essentially|fundamentally|technically|in fact)\b',
            r'^(the key|the important|what happens|you see|think of it|imagine)\b',
            r'^(for example|specifically|in this case|when we|if you)\b',
            r'^(it\'s about|it depends|the reason|because|since|due to)\b',
            r'\b(algorithm|function|variable|data|code|program|implementation)\b',  # Technical terms
        ]
        
        # Check patterns
        if any(re.search(pattern, text_lower) for pattern in host_patterns):
            return speaker_names[0]  # Dr. James (host)
        elif any(re.search(pattern, text_lower) for pattern in expert_patterns):
            return speaker_names[1]  # Sarah (expert)
        
        # Context-based detection - look at conversation flow
        if prev_context:
            last_segment = prev_context[-1]
            
            # If previous was a question from host, this is likely expert response
            if last_segment['speaker'] == speaker_names[0] and '?' in last_segment['text']:
                # This should be the expert responding
                return speaker_names[1]
            
            # If previous was a long explanation from expert, short response is likely host
            if (last_segment['speaker'] == speaker_names[1] and 
                len(last_segment['text']) > 50 and len(text) < 30):
                return speaker_names[0]
        
        return None
    
    def _detect_speaker_pattern(self, text: str, current_speaker: str, speaker_names: Tuple[str, str]) -> Optional[str]:
        """Legacy method - redirect to enhanced version"""
        return self._detect_speaker_pattern_enhanced(text, current_speaker, speaker_names, [])
    
    def create_clean_conversation(self, segments: List[Dict], video_info: Dict, 
                                max_speaker_length: int = 800) -> str:
        """Create clean conversational script with grouped consecutive speakers for better TTS"""
        print("‚úçÔ∏è Creating clean conversation with speaker grouping...")
        
        conversation = ["<!--"]
        
        # Group consecutive same-speaker segments
        grouped_segments = []
        current_speaker = None
        current_texts = []
        
        for segment in segments:
            speaker = segment['speaker']
            text = segment['text'].strip()
            
            if not text:
                continue
            
            # Clean the text
            clean_text = self._clean_text(text)
            if not clean_text:
                continue
            
            # If same speaker, accumulate text
            if speaker == current_speaker:
                current_texts.append(clean_text)
            else:
                # Save previous speaker's accumulated text
                if current_speaker and current_texts:
                    combined_text = ' '.join(current_texts)
                    grouped_segments.append({
                        'speaker': current_speaker,
                        'text': combined_text
                    })
                
                # Start new speaker group
                current_speaker = speaker
                current_texts = [clean_text]
        
        # Don't forget the final group
        if current_speaker and current_texts:
            combined_text = ' '.join(current_texts)
            grouped_segments.append({
                'speaker': current_speaker,
                'text': combined_text
            })
        
        # Generate conversation with grouped speakers
        for group in grouped_segments:
            speaker = group['speaker']
            text = group['text']
            
            # Apply length management if needed for very long paragraphs
            if len(text) > max_speaker_length:
                split_parts = self._split_long_text(text, max_speaker_length)
                for i, part in enumerate(split_parts):
                    if part.strip():
                        if i == 0:
                            conversation.append(f"{speaker}: {part.strip()}")
                        else:
                            # Continue same speaker on new line for readability
                            conversation.append(f"{speaker}: {part.strip()}")
                        conversation.append("")
            else:
                conversation.append(f"{speaker}: {text}")
                conversation.append("")
        
        conversation.append("-->")
        
        result = '\n'.join(conversation)
        print(f"‚úÖ Clean conversation created with grouped speakers ({len(result)} characters)")
        return result
    
    def _clean_text(self, text: str) -> str:
        """Clean text from audio transcription artifacts"""
        if not text:
            return ""
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove immediate word repetitions (transcription artifacts)
        text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text)  # word word -> word
        text = re.sub(r'\b(\w+)([.!?])\s+\1\2', r'\1\2', text)  # word. word. -> word.
        text = re.sub(r'\b(\w+),\s+\1\b', r'\1', text)  # word, word -> word
        
        # Clean up common transcription artifacts
        text = re.sub(r'\[.*?\]', '', text)  # Remove [inaudible] etc.
        text = re.sub(r'\(.*?\)', '', text)  # Remove (background noise) etc.
        
        return text.strip()
    
    def _split_long_text(self, text: str, max_length: int) -> List[str]:
        """Split long text at natural sentence boundaries"""
        if len(text) <= max_length:
            return [text]
        
        # Split by sentences
        sentences = re.split(r'([.!?;])', text)
        
        # Recombine with punctuation
        sentence_parts = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                sentence = sentences[i] + sentences[i + 1]
                if sentence.strip():
                    sentence_parts.append(sentence.strip())
        
        # Group sentences within length limit
        result = []
        current_part = []
        current_length = 0
        
        for sentence in sentence_parts:
            sentence_length = len(sentence)
            
            if current_length + sentence_length > max_length and current_part:
                result.append(' '.join(current_part))
                current_part = [sentence]
                current_length = sentence_length
            else:
                current_part.append(sentence)
                current_length += sentence_length + 1
        
        if current_part:
            result.append(' '.join(current_part))
        
        return result
    
    def save_clean_script(self, script: str, video_info: Dict) -> Path:
        """Save clean script to file"""
        safe_title = re.sub(r'[^\w\s-]', '', video_info['title'])[:50]
        safe_title = re.sub(r'[-\s]+', '-', safe_title).lower()
        
        output_file = self.output_dir / f"{safe_title}-audio-clean.txt"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script)
        
        print(f"‚úÖ Clean script saved: {output_file}")
        return output_file
    
    def save_timestamped_transcript(self, segments: List[Dict], video_info: Dict) -> Path:
        """Save transcript with precise word-level timestamps for video synchronization"""
        safe_title = re.sub(r'[^\w\s-]', '', video_info['title'])[:50]
        safe_title = re.sub(r'[-\s]+', '-', safe_title).lower()
        
        output_file = self.output_dir / f"{safe_title}-timestamped.json"
        
        # Create detailed timestamp data
        timestamped_data = {
            'metadata': {
                'title': video_info['title'],
                'duration': video_info.get('duration', 0),
                'generated_at': datetime.now().isoformat(),
                'format': 'word_level_timestamps'
            },
            'segments': []
        }
        
        for segment in segments:
            segment_data = {
                'speaker': segment['speaker'],
                'text': segment['text'],
                'start': segment['start'],
                'end': segment['end'],
                'duration': segment['end'] - segment['start']
            }
            timestamped_data['segments'].append(segment_data)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(timestamped_data, f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ Timestamped transcript saved: {output_file}")
        return output_file
    
    def cleanup(self):
        """Clean up temporary files"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
            print(f"üßπ Cleaned up: {self.temp_dir}")
        except Exception:
            pass
    
    def is_audio_file(self, input_path: str) -> bool:
        """Check if input is an audio file path"""
        # First check if it's clearly a URL
        if input_path.startswith(('http://', 'https://', 'www.')):
            return False
        
        # Check if it's a file path (either absolute or relative)
        file_path = Path(input_path)
        
        # If it has an audio file extension, consider it a file
        audio_extensions = {'.m4a', '.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma'}
        if file_path.suffix.lower() in audio_extensions:
            return True
        
        # If it exists as a file, consider it a file
        if file_path.exists() and file_path.is_file():
            return True
        
        return False
    
    def get_audio_info_from_file(self, audio_path: str) -> Dict:
        """Extract basic info from audio file"""
        file_path = Path(audio_path)
        
        return {
            'title': file_path.stem.replace('-', ' ').replace('_', ' ').title(),
            'uploader': 'Local File',
            'duration': 0,  # Could use librosa to get duration if needed
            'video_id': file_path.stem
        }
    
    def prepare_audio_file(self, audio_path: str) -> Optional[Path]:
        """Prepare audio file (copy to temp dir with correct format)"""
        print(f"üéµ Preparing audio file...")
        
        source_path = Path(audio_path)
        if not source_path.exists():
            print(f"‚ùå Audio file not found: {audio_path}")
            return None
        
        # Copy to temp directory with wav extension for Whisper
        audio_file = self.temp_dir / f"{source_path.stem}.wav"
        
        try:
            # Convert to wav format using ffmpeg if needed
            if source_path.suffix.lower() in ['.m4a', '.mp3', '.flac', '.aac']:
                print(f"üîÑ Converting {source_path.suffix} to WAV format...")
                import subprocess
                result = subprocess.run([
                    'ffmpeg', '-i', str(source_path), 
                    '-ar', '16000',  # 16kHz sample rate for Whisper
                    '-ac', '1',      # Mono
                    '-y',            # Overwrite output
                    str(audio_file)
                ], capture_output=True, text=True)
                
                if result.returncode != 0:
                    print(f"‚ùå Audio conversion failed: {result.stderr}")
                    # Try copying directly if conversion fails
                    import shutil
                    shutil.copy2(source_path, audio_file.with_suffix(source_path.suffix))
                    audio_file = audio_file.with_suffix(source_path.suffix)
            else:
                # Copy directly if already in compatible format
                import shutil
                shutil.copy2(source_path, audio_file.with_suffix(source_path.suffix))
                audio_file = audio_file.with_suffix(source_path.suffix)
            
            print(f"‚úÖ Audio file ready: {audio_file.name}")
            return audio_file
            
        except Exception as e:
            print(f"‚ùå Audio preparation failed: {e}")
            return None
    
    def generate_audio_script(self, input_source: str, speaker_names: Tuple[str, str] = ("Dr. James", "Sarah")) -> Optional[Path]:
        """Main pipeline for audio-based script generation from URL or file"""
        is_file = self.is_audio_file(input_source)
        
        print(f"üöÄ Generating AUDIO-BASED clean two-speaker script")
        print(f"   Source: {'Audio File' if is_file else 'YouTube URL'}")
        print(f"   Input: {input_source}")
        print(f"   Speakers: {speaker_names[0]} & {speaker_names[1]}")
        print(f"   Method: Whisper + Speaker Diarization")
        print("=" * 60)
        
        if not WHISPER_AVAILABLE:
            print("‚ùå Whisper not available")
            return None
        
        if not is_file and not YTDLP_AVAILABLE:
            print("‚ùå yt-dlp required for YouTube URLs")
            return None
        
        try:
            if is_file:
                # Handle direct audio file
                audio_info = self.get_audio_info_from_file(input_source)
                print(f"üìÅ Processing audio file...")
                print(f"‚úÖ {audio_info['title']}")
                
                audio_file = self.prepare_audio_file(input_source)
                if not audio_file:
                    return None
            else:
                # Handle YouTube URL
                audio_info = self.extract_video_info(input_source)
                if not audio_info:
                    return None
                
                audio_file = self.download_audio(input_source, audio_info)
                if not audio_file:
                    return None
            
            # Transcribe with Whisper (word-level timestamps)
            transcription = self.transcribe_with_whisper(audio_file)
            if not transcription:
                return None
            
            # Perform speaker diarization
            diarization = self.perform_speaker_diarization(audio_file)
            
            # Align speakers with transcription
            aligned_segments = self.align_speakers_with_transcription(
                transcription, diarization, speaker_names
            )
            
            # Create clean conversation
            conversation = self.create_clean_conversation(aligned_segments, audio_info)
            
            # Save script
            output_file = self.save_clean_script(conversation, audio_info)
            
            # Save timestamped transcript for video synchronization
            timestamped_file = self.save_timestamped_transcript(aligned_segments, audio_info)
            
            print(f"\nüéâ SUCCESS! Audio-based clean script generated")
            print(f"üìÅ Output: {output_file}")
            print(f"‚è±Ô∏è  Timestamped: {timestamped_file}")
            print(f"‚ú® Method: Direct audio processing with Whisper")
            print(f"üéØ Accuracy: Significantly higher than caption-based approach")
            
            return output_file
            
        except Exception as e:
            print(f"‚ùå Generation failed: {e}")
            return None
        finally:
            self.cleanup()

def main():
    parser = argparse.ArgumentParser(
        description="Generate ultra-clean two-speaker scripts from YouTube URLs or audio files"
    )
    
    parser.add_argument('input', help='YouTube video URL or audio file path (.m4a, .mp3, .wav, etc.)')
    parser.add_argument('--speakers', help='Speaker names (comma-separated, default: "Dr. James,Sarah")')
    parser.add_argument('--output-dir', default='audio_scripts', help='Output directory')
    
    args = parser.parse_args()
    
    # Parse speakers
    if args.speakers:
        speaker_names = tuple(name.strip() for name in args.speakers.split(','))
        if len(speaker_names) != 2:
            print("‚ùå Need exactly 2 speaker names")
            return 1
    else:
        speaker_names = ("Dr. James", "Sarah")
    
    # Generate
    generator = YouTubeAudioScriptGenerator(args.output_dir)
    result = generator.generate_audio_script(args.input, speaker_names)
    
    if result:
        print(f"\n‚ú® Perfect! Audio-based script provides superior accuracy!")
        return 0
    else:
        return 1

if __name__ == "__main__":
    sys.exit(main())